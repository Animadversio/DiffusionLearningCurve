{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Score vs Denoising Score Matching Comparison\n",
    "\n",
    "This notebook demonstrates the difference between:\n",
    "1. **Ground Truth Score Learning**: Training a neural network to directly predict the analytical score function ∇_x log p(x)\n",
    "2. **Denoising Score Matching**: Training via the EDM denoising objective (standard diffusion training)\n",
    "\n",
    "We'll use a Gaussian mixture model where we can compute the exact score analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionLearningCurve\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core.gaussian_mixture_lib import GaussianMixture\n",
    "from core.gmm_general_diffusion_lib import gaussian_mixture_score_torch\n",
    "from core.diffusion_edm_lib import (\n",
    "    UNetBlockStyleMLP_backbone, \n",
    "    EDMPrecondWrapper,\n",
    "    EDMLoss,\n",
    "    train_score_model_custom_loss,\n",
    "    edm_sampler\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create a Gaussian Mixture Model\n",
    "\n",
    "We'll create a simple 2D Gaussian mixture with known analytical score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2D Gaussian mixture\n",
    "mus = [np.array([-2.0, -1.0]), np.array([2.0, 1.0]), np.array([0.0, 2.5])]\n",
    "covs = [np.array([[0.8, 0.2], [0.2, 0.8]]), \n",
    "        np.array([[1.2, -0.4], [-0.4, 1.2]]),\n",
    "        np.array([[0.6, 0.0], [0.0, 0.6]])]\n",
    "weights = [0.4, 0.4, 0.2]\n",
    "\n",
    "gmm = GaussianMixture(mus, covs, weights)\n",
    "\n",
    "# Generate training data\n",
    "n_samples = 5000\n",
    "X_train, components, _ = gmm.sample(n_samples)\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "# Visualize the data and analytical score field\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot data\n",
    "ax1.scatter(X_train[:, 0], X_train[:, 1], alpha=0.6, s=10)\n",
    "ax1.set_title('Training Data from GMM')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Plot analytical score field\n",
    "x_range = np.linspace(-4, 4, 20)\n",
    "y_range = np.linspace(-3, 4, 20)\n",
    "XX, YY = np.meshgrid(x_range, y_range)\n",
    "grid_points = np.stack([XX.flatten(), YY.flatten()], axis=1)\n",
    "true_scores = gmm.score(grid_points)\n",
    "\n",
    "ax2.quiver(XX, YY, true_scores[:, 0].reshape(XX.shape), \n",
    "           true_scores[:, 1].reshape(XX.shape), alpha=0.7)\n",
    "ax2.set_title('True Score Field ∇log p(x)')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax2.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {n_samples} samples from {gmm.n_component}-component GMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Ground Truth Score Learning\n",
    "\n",
    "Train a neural network to directly predict the analytical score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthScoreLoss:\n",
    "    \"\"\"Loss function for direct score matching against analytical ground truth\"\"\"\n",
    "    def __init__(self, gmm):\n",
    "        self.gmm = gmm\n",
    "    \n",
    "    def __call__(self, model, X):\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        true_scores = self.gmm.score(X_np)\n",
    "        true_scores_torch = torch.tensor(true_scores, dtype=torch.float32, device=X.device)\n",
    "        \n",
    "        # Dummy time input (not used for ground truth score)\n",
    "        t_dummy = torch.zeros(X.shape[0], device=X.device)\n",
    "        pred_scores = model(X, t_dummy)\n",
    "        \n",
    "        loss = F.mse_loss(pred_scores, true_scores_torch, reduction='none')\n",
    "        return loss\n",
    "\n",
    "# Create model for ground truth score learning\n",
    "gt_model = UNetBlockStyleMLP_backbone(ndim=2, nlayers=4, nhidden=64, time_embed_dim=32)\n",
    "gt_loss_fn = GroundTruthScoreLoss(gmm)\n",
    "\n",
    "print(\"Training Ground Truth Score Model...\")\n",
    "gt_model_trained, gt_loss_traj = train_score_model_custom_loss(\n",
    "    X_train_torch, gt_model, gt_loss_fn,\n",
    "    lr=0.001, nepochs=1000, batch_size=512, device=device\n",
    ")\n",
    "\n",
    "print(f\"Ground truth model final loss: {gt_loss_traj[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Denoising Score Matching (EDM)\n",
    "\n",
    "Train using standard EDM denoising objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for denoising score matching\n",
    "dsm_model = UNetBlockStyleMLP_backbone(ndim=2, nlayers=4, nhidden=64, time_embed_dim=32)\n",
    "dsm_model_precd = EDMPrecondWrapper(dsm_model, sigma_data=0.5)\n",
    "edm_loss_fn = EDMLoss(P_mean=-1.2, P_std=1.2, sigma_data=0.5)\n",
    "\n",
    "print(\"Training Denoising Score Matching Model...\")\n",
    "dsm_model_trained, dsm_loss_traj = train_score_model_custom_loss(\n",
    "    X_train_torch, dsm_model_precd, edm_loss_fn,\n",
    "    lr=0.001, nepochs=1000, batch_size=512, device=device\n",
    ")\n",
    "\n",
    "print(f\"Denoising model final loss: {dsm_loss_traj[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Loss Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gt_loss_traj, label='Ground Truth Score Loss', alpha=0.8)\n",
    "plt.plot(dsm_loss_traj, label='Denoising Score Matching Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gt_loss_traj[100:], label='Ground Truth Score Loss', alpha=0.8)\n",
    "plt.plot(dsm_loss_traj[100:], label='Denoising Score Matching Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (after epoch 100)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Score Field Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on a grid\n",
    "with torch.no_grad():\n",
    "    grid_torch = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "    t_dummy = torch.zeros(grid_torch.shape[0], device=device)\n",
    "    \n",
    "    # Ground truth model predictions\n",
    "    gt_pred_scores = gt_model_trained(grid_torch, t_dummy).cpu().numpy()\n",
    "    \n",
    "    # Denoising model predictions (at σ=0, which should give the score)\n",
    "    sigma_eval = torch.full((grid_torch.shape[0],), 0.01, device=device)  # Very small σ\n",
    "    dsm_pred_clean = dsm_model_trained(grid_torch, sigma_eval).cpu().numpy()\n",
    "    # Convert from denoised prediction to score: score = -(x_noisy - x_clean) / σ²\n",
    "    dsm_pred_scores = -(grid_torch.cpu().numpy() - dsm_pred_clean) / (0.01**2)\n",
    "\n",
    "# Visualize score field comparisons\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# True scores\n",
    "axes[0,0].quiver(XX, YY, true_scores[:, 0].reshape(XX.shape), \n",
    "                 true_scores[:, 1].reshape(XX.shape), alpha=0.7)\n",
    "axes[0,0].set_title('True Score Field')\n",
    "axes[0,0].axis('equal')\n",
    "\n",
    "# Ground truth model scores\n",
    "axes[0,1].quiver(XX, YY, gt_pred_scores[:, 0].reshape(XX.shape), \n",
    "                 gt_pred_scores[:, 1].reshape(XX.shape), alpha=0.7, color='red')\n",
    "axes[0,1].set_title('Ground Truth Model Predictions')\n",
    "axes[0,1].axis('equal')\n",
    "\n",
    "# Denoising model scores\n",
    "axes[0,2].quiver(XX, YY, dsm_pred_scores[:, 0].reshape(XX.shape), \n",
    "                 dsm_pred_scores[:, 1].reshape(XX.shape), alpha=0.7, color='green')\n",
    "axes[0,2].set_title('Denoising Model Predictions')\n",
    "axes[0,2].axis('equal')\n",
    "\n",
    "# Error maps\n",
    "gt_error = np.linalg.norm(gt_pred_scores - true_scores, axis=1)\n",
    "dsm_error = np.linalg.norm(dsm_pred_scores - true_scores, axis=1)\n",
    "\n",
    "im1 = axes[1,0].scatter(grid_points[:, 0], grid_points[:, 1], c=gt_error, cmap='viridis', s=20)\n",
    "axes[1,0].set_title('Ground Truth Model Error')\n",
    "plt.colorbar(im1, ax=axes[1,0])\n",
    "axes[1,0].axis('equal')\n",
    "\n",
    "im2 = axes[1,1].scatter(grid_points[:, 0], grid_points[:, 1], c=dsm_error, cmap='viridis', s=20)\n",
    "axes[1,1].set_title('Denoising Model Error')\n",
    "plt.colorbar(im2, ax=axes[1,1])\n",
    "axes[1,1].axis('equal')\n",
    "\n",
    "# Error comparison\n",
    "axes[1,2].hist(gt_error, alpha=0.7, label=f'GT Model (mean: {gt_error.mean():.3f})', bins=30)\n",
    "axes[1,2].hist(dsm_error, alpha=0.7, label=f'DSM Model (mean: {dsm_error.mean():.3f})', bins=30)\n",
    "axes[1,2].set_xlabel('Score Prediction Error')\n",
    "axes[1,2].set_ylabel('Count')\n",
    "axes[1,2].set_title('Error Distribution')\n",
    "axes[1,2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ground Truth Model Mean Error: {gt_error.mean():.6f}\")\n",
    "print(f\"Denoising Model Mean Error: {dsm_error.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Quality Comparison\n",
    "\n",
    "Compare the quality of samples generated by both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples using both models\n",
    "n_gen_samples = 2000\n",
    "\n",
    "# For ground truth model, we need to implement a simple Langevin sampler\n",
    "def langevin_sampler(score_model, n_samples, n_steps=1000, step_size=0.01, init_noise_scale=2.0):\n",
    "    \"\"\"Simple Langevin dynamics sampler for ground truth score model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Initialize with noise\n",
    "        x = torch.randn(n_samples, 2, device=device) * init_noise_scale\n",
    "        t_dummy = torch.zeros(n_samples, device=device)\n",
    "        \n",
    "        for i in tqdm(range(n_steps), desc=\"Langevin sampling\"):\n",
    "            score = score_model(x, t_dummy)\n",
    "            noise = torch.randn_like(x) * np.sqrt(2 * step_size)\n",
    "            x = x + step_size * score + noise\n",
    "            \n",
    "    return x.cpu().numpy()\n",
    "\n",
    "print(\"Sampling from Ground Truth Model...\")\n",
    "gt_samples = langevin_sampler(gt_model_trained, n_gen_samples, n_steps=500)\n",
    "\n",
    "print(\"Sampling from Denoising Model...\")\n",
    "with torch.no_grad():\n",
    "    noise_init = torch.randn(n_gen_samples, 2, device=device)\n",
    "    dsm_samples = edm_sampler(\n",
    "        dsm_model_trained, noise_init, \n",
    "        num_steps=50, sigma_min=0.002, sigma_max=80, rho=7\n",
    "    ).cpu().numpy()\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], alpha=0.6, s=10)\n",
    "axes[0].set_title('Original Training Data')\n",
    "axes[0].axis('equal')\n",
    "axes[0].set_xlim(-5, 5)\n",
    "axes[0].set_ylim(-4, 5)\n",
    "\n",
    "# Ground truth model samples\n",
    "axes[1].scatter(gt_samples[:, 0], gt_samples[:, 1], alpha=0.6, s=10, color='red')\n",
    "axes[1].set_title('Ground Truth Model Samples')\n",
    "axes[1].axis('equal')\n",
    "axes[1].set_xlim(-5, 5)\n",
    "axes[1].set_ylim(-4, 5)\n",
    "\n",
    "# Denoising model samples\n",
    "axes[2].scatter(dsm_samples[:, 0], dsm_samples[:, 1], alpha=0.6, s=10, color='green')\n",
    "axes[2].set_title('Denoising Model Samples')\n",
    "axes[2].axis('equal')\n",
    "axes[2].set_xlim(-5, 5)\n",
    "axes[2].set_ylim(-4, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def compute_sample_metrics(true_samples, gen_samples):\n",
    "    \"\"\"Compute various metrics to compare sample quality\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Mean and covariance comparison\n",
    "    true_mean = np.mean(true_samples, axis=0)\n",
    "    gen_mean = np.mean(gen_samples, axis=0)\n",
    "    metrics['mean_error'] = np.linalg.norm(true_mean - gen_mean)\n",
    "    \n",
    "    true_cov = np.cov(true_samples.T)\n",
    "    gen_cov = np.cov(gen_samples.T)\n",
    "    metrics['cov_frobenius_error'] = np.linalg.norm(true_cov - gen_cov, 'fro')\n",
    "    \n",
    "    # 2. Wasserstein distances (1D marginals)\n",
    "    metrics['wasserstein_x'] = wasserstein_distance(true_samples[:, 0], gen_samples[:, 0])\n",
    "    metrics['wasserstein_y'] = wasserstein_distance(true_samples[:, 1], gen_samples[:, 1])\n",
    "    \n",
    "    # 3. Nearest neighbor distances (coverage)\n",
    "    dists_true_to_gen = cdist(true_samples, gen_samples)\n",
    "    min_dists_coverage = np.min(dists_true_to_gen, axis=1)\n",
    "    metrics['coverage_mean'] = np.mean(min_dists_coverage)\n",
    "    \n",
    "    # 4. Precision (how close generated samples are to true samples)\n",
    "    dists_gen_to_true = cdist(gen_samples, true_samples)\n",
    "    min_dists_precision = np.min(dists_gen_to_true, axis=1)\n",
    "    metrics['precision_mean'] = np.mean(min_dists_precision)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "gt_metrics = compute_sample_metrics(X_train, gt_samples)\n",
    "dsm_metrics = compute_sample_metrics(X_train, dsm_samples)\n",
    "\n",
    "print(\"Sample Quality Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<20} {'GT Model':<15} {'DSM Model':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for key in gt_metrics.keys():\n",
    "    print(f\"{key:<20} {gt_metrics[key]:<15.6f} {dsm_metrics[key]:<15.6f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary:\")\n",
    "print(\"=\" * 30)\n",
    "if gt_metrics['precision_mean'] < dsm_metrics['precision_mean']:\n",
    "    print(\"✓ Ground Truth model has better precision (samples closer to true data)\")\n",
    "else:\n",
    "    print(\"✓ Denoising model has better precision (samples closer to true data)\")\n",
    "    \n",
    "if gt_metrics['coverage_mean'] < dsm_metrics['coverage_mean']:\n",
    "    print(\"✓ Ground Truth model has better coverage (true data better covered)\")\n",
    "else:\n",
    "    print(\"✓ Denoising model has better coverage (true data better covered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Conclusions\n",
    "\n",
    "**Ground Truth Score Learning:**\n",
    "- Directly learns the score function ∇ log p(x)\n",
    "- Requires analytical score (only available for simple distributions)\n",
    "- Can achieve very low prediction error on the score field\n",
    "- Sampling requires Langevin dynamics or other MCMC methods\n",
    "\n",
    "**Denoising Score Matching (EDM):**\n",
    "- Learns score implicitly through denoising at multiple noise levels\n",
    "- Works for any distribution (no analytical score needed)\n",
    "- Provides efficient deterministic sampling via ODE/SDE solvers\n",
    "- More robust and scalable to high-dimensional data\n",
    "\n",
    "**Comparison:**\n",
    "- Ground truth learning can be more accurate when analytical scores are available\n",
    "- Denoising score matching is more practical and generalizable\n",
    "- Both approaches can generate high-quality samples when properly trained\n",
    "- The choice depends on whether analytical scores are available and computational constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}