{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9ca504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eab3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import trange, tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from easydict import EasyDict as edict\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionLearningCurve\")\n",
    "sys.path.append(\"/Users/binxuwang/Github/DiffusionLearningCurve/\")\n",
    "from core.diffusion_nn_lib import UNetBlockStyleMLP_backbone\n",
    "from core.toy_shape_dataset_lib import generate_random_star_shape_torch\n",
    "from core.diffusion_basics_lib import *\n",
    "from core.diffusion_edm_lib import *\n",
    "from core.network_edm_lib import SongUNet, DhariwalUNet\n",
    "from core.DiT_model_lib import *\n",
    "from core.diffusion_nn_lib import UNetBlockStyleMLP_backbone\n",
    "from circuit_toolkit.plot_utils import saveallforms, to_imgrid, show_imgrid\n",
    "from pprint import pprint\n",
    "\n",
    "saveroot = f\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c26c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_ckpt_step(ckptdir, verbose=True):\n",
    "    ckpt_files = [f for f in os.listdir(ckptdir) if f.startswith(\"model_epoch_\") and f.endswith(\".pth\")]\n",
    "    ckpt_steps = [int(f.split(\"_\")[-1].split(\".\")[0]) for f in ckpt_files]\n",
    "    if len(ckpt_steps) == 0:\n",
    "        if verbose:\n",
    "            print(\"No checkpoints found in the directory! check the path: \", ckptdir)\n",
    "        return None\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Found {len(ckpt_steps)} checkpoints in the directory, largest step is {max(ckpt_steps)}\")\n",
    "        return max(ckpt_steps)\n",
    "\n",
    "\n",
    "def find_all_ckpt_steps(ckptdir, verbose=True):\n",
    "    ckpt_files = [f for f in os.listdir(ckptdir) if f.startswith(\"model_epoch_\") and f.endswith(\".pth\")]\n",
    "    ckpt_steps = [int(f.split(\"_\")[-1].split(\".\")[0]) for f in ckpt_files]\n",
    "    if verbose:\n",
    "        print(f\"Found {len(ckpt_steps)} checkpoints in the directory, largest step is {max(ckpt_steps)}\")\n",
    "    return sorted(ckpt_steps)\n",
    "\n",
    "\n",
    "# Refactored functions for visualizing gradient maps\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "def compute_gradient_map(model, x_shape, sigma_val, output_coords, device, x_probe=None, target=\"denoiser\"):\n",
    "    \"\"\"\n",
    "    Compute gradient map for a specific sigma value and output coordinates.\n",
    "    \n",
    "    Args:\n",
    "        model: The denoiser model\n",
    "        x_shape: Shape of the input tensor (e.g., (3, 32, 32))\n",
    "        sigma_val: Sigma value for the denoiser\n",
    "        output_coords: Tuple of (channel, y, x) coordinates for the output pixel\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        Gradient map tensor\n",
    "    \"\"\"\n",
    "    # Set up the output map (focusing on the specified pixel)\n",
    "    output_map = torch.zeros(x_shape).to(device)\n",
    "    if output_coords is None:\n",
    "        # Default to center pixel if not specified\n",
    "        c, h, w = x_shape\n",
    "        output_map[:, h//2, w//2] = 1\n",
    "    else:\n",
    "        c, y, x = output_coords\n",
    "        if c is None:  # If channel is None, set all channels\n",
    "            output_map[:, y, x] = 1\n",
    "        else:\n",
    "            output_map[c, y, x] = 1\n",
    "    \n",
    "    # Set up the probe point\n",
    "    if x_probe is None:\n",
    "        x_probe = sigma_val * torch.randn(x_shape).to(device)\n",
    "    else:\n",
    "        x_probe = x_probe.detach().clone().to(device)\n",
    "    x_probe.requires_grad_(True)\n",
    "    \n",
    "    # Compute the denoiser output\n",
    "    t_sigma = sigma_val * torch.ones(1, device=device)\n",
    "    denoised = model(x_probe.view(1, *x_shape), t_sigma)\n",
    "    if target == \"denoiser\":\n",
    "        scalar = (output_map * denoised).sum()\n",
    "    elif target == \"score\":\n",
    "        score = (denoised - x_probe.view(1, *x_shape)) / t_sigma[:, None]\n",
    "        scalar = (output_map * score[0]).sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid target: {target}\")\n",
    "    \n",
    "    # Compute scalar output and get gradient\n",
    "    scalar.backward()\n",
    "    # Return the gradient\n",
    "    return x_probe.grad.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_gradient_maps(model, x_shape, sigma_values, output_coords=None, target=\"denoiser\",\n",
    "                           device='cuda', reduction='abs_mean', figsize=(14, 14)):\n",
    "    \"\"\"\n",
    "    Visualize gradient maps for different sigma values.\n",
    "    \n",
    "    Args:\n",
    "        model: The denoiser model\n",
    "        x_shape: Shape of the input tensor (e.g., (3, 32, 32))\n",
    "        sigma_values: List of sigma values to test\n",
    "        output_coords: Tuple of (channel, y, x) coordinates for the output pixel\n",
    "                      If None, defaults to center pixel\n",
    "        device: Device to run computation on\n",
    "        reduction: How to reduce channel dimension ('abs_mean', 'mean', 'max', or None)\n",
    "        figsize: Figure size for the plot\n",
    "    \n",
    "    Returns:\n",
    "        List of computed gradient maps\n",
    "    \"\"\"\n",
    "    gradient_maps = []\n",
    "    \n",
    "    # Compute gradient maps for each sigma\n",
    "    for sigma_val in sigma_values:\n",
    "        print(f\"Computing gradient map for sigma = {sigma_val}\")\n",
    "        gradient_map = compute_gradient_map(model, x_shape, sigma_val, output_coords, device, target=target)\n",
    "        \n",
    "        # Apply reduction if specified\n",
    "        if reduction == 'abs_mean':\n",
    "            gradient_map = gradient_map.abs().mean(0)\n",
    "        elif reduction == 'mean':\n",
    "            gradient_map = gradient_map.mean(0)\n",
    "        elif reduction == 'max':\n",
    "            gradient_map = gradient_map.abs().max(0)[0]\n",
    "        # If None, keep all channels\n",
    "        \n",
    "        gradient_maps.append(gradient_map)\n",
    "        # Clear gradients for next iteration\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create a montage of the gradient maps\n",
    "    rows = int(len(sigma_values)**0.5)\n",
    "    cols = (len(sigma_values) + rows - 1) // rows  # Ceiling division\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                    nrows_ncols=(rows, cols),\n",
    "                    axes_pad=0.3,\n",
    "                    share_all=True,\n",
    "                    cbar_location=\"right\",\n",
    "                    cbar_mode=\"single\",\n",
    "                    cbar_size=\"5%\",\n",
    "                    cbar_pad=0.1)\n",
    "    \n",
    "    # Add each gradient map to the grid\n",
    "    for i, (gradient_map, sigma_val) in enumerate(zip(gradient_maps, sigma_values)):\n",
    "        if i < len(grid):  # Ensure we don't go out of bounds\n",
    "            ax = grid[i]\n",
    "            \n",
    "            # Handle multi-channel gradient maps\n",
    "            if len(gradient_map.shape) == 3 and reduction is None:\n",
    "                # Just show first channel if no reduction\n",
    "                im = ax.imshow(gradient_map[0], cmap=\"viridis\")\n",
    "            else:\n",
    "                im = ax.imshow(gradient_map, cmap=\"viridis\")\n",
    "                \n",
    "            ax.set_title(f\"Ïƒ = {sigma_val}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    # Add colorbar\n",
    "    grid.cbar_axes[0].colorbar(im)\n",
    "    \n",
    "    # Add title with output coordinates\n",
    "    if output_coords:\n",
    "        c, y, x = output_coords\n",
    "        channel_str = f\"channel {c}\" if c is not None else \"all channels\"\n",
    "        plt.suptitle(f\"Gradient Maps of {target} for Different Sigma Values (Output at {channel_str}, y={y}, x={x})\")\n",
    "    else:\n",
    "        plt.suptitle(f\"Gradient Maps of {target} for Different Sigma Values (Output at center pixel)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, gradient_maps\n",
    "\n",
    "\n",
    "def compute_jacobian(model, x_probe, sigma, device=\"cuda\", target=\"denoiser\", pbar=True):\n",
    "    x_shape = x_probe.shape\n",
    "    jacobian = torch.zeros(np.prod(x_shape[:]), np.prod(x_shape[:])).to(device)\n",
    "    if pbar:\n",
    "        pbar = trange(np.prod(x_shape[:]))\n",
    "    else:\n",
    "        pbar = range(np.prod(x_shape[:]))\n",
    "    for idx in pbar:\n",
    "        coords = np.unravel_index(idx, x_shape[:])\n",
    "        grad = compute_gradient_map(model, x_shape, sigma, coords, device, x_probe=x_probe, target=target)\n",
    "        jacobian[idx, :] = grad.view(-1)\n",
    "    return jacobian\n",
    "\n",
    "\n",
    "# %%\n",
    "# %% [markdown]\n",
    "# ### Loading CNN\n",
    "#%%\n",
    "def create_unet_model(config):\n",
    "    unet = SongUNet(in_channels=config.channels, \n",
    "                out_channels=config.channels, \n",
    "                num_blocks=config.layers_per_block, \n",
    "                attn_resolutions=config.attn_resolutions, \n",
    "                decoder_init_attn=config.decoder_init_attn if 'decoder_init_attn' in config else True,\n",
    "                model_channels=config.model_channels, \n",
    "                channel_mult=config.channel_mult, \n",
    "                dropout=config.dropout, \n",
    "                img_resolution=config.img_size, \n",
    "                label_dim=config.label_dim,\n",
    "                embedding_type='positional', \n",
    "                encoder_type='standard', \n",
    "                decoder_type='standard', \n",
    "                augment_dim=config.augment_dim, #  no augmentation , 9 for defaults. \n",
    "                channel_mult_noise=1, \n",
    "                resample_filter=[1,1], \n",
    "                )\n",
    "    pytorch_total_grad_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f'total number of trainable parameters in the Score Model: {pytorch_total_grad_params}')\n",
    "    pytorch_total_params = sum(p.numel() for p in unet.parameters())\n",
    "    print(f'total number of parameters in the Score Model: {pytorch_total_params}')\n",
    "    return unet\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6594b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_MLP_EDM\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_MLP_EDM_width1024_small_lr_long_train\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_1block_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_1block_wide128_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_2blocks_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_noattn_denser\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_noattn_smalllr\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_noattn_smalllr_longtrain\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_wide64_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_deeper_1block_noattn\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_shallow\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_shallow_1block_noattn\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST* -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d75f0",
   "metadata": {},
   "source": [
    "### UNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ef31384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attn_resolutions': [0],\n",
      " 'augment_dim': 0,\n",
      " 'channel_mult': [1, 2],\n",
      " 'channels': 1,\n",
      " 'dropout': 0.0,\n",
      " 'img_size': 32,\n",
      " 'label_dim': 0,\n",
      " 'layers_per_block': 2,\n",
      " 'model_channels': 32}\n",
      "total number of trainable parameters in the Score Model: 982049\n",
      "total number of parameters in the Score Model: 982049\n",
      "load ckpt from  /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_shallow/model_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1275571/516785737.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(ckpt_path))\n"
     ]
    }
   ],
   "source": [
    "# loading config \n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_noattn\"\n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_wide64_noattn\"\n",
    "expname = \"MNIST_UNet_CNN_EDM_shallow\"\n",
    "savedir = join(saveroot, expname)\n",
    "ckptdir = join(savedir, \"ckpts\")\n",
    "sample_dir = join(savedir, \"samples\")\n",
    "config = edict(json.load(open(f\"{savedir}/config.json\")))\n",
    "# args = edict(json.load(open(f\"{savedir}/args.json\")))\n",
    "pprint(config)\n",
    "unet = create_unet_model(config)\n",
    "# ckpt_step_list = find_all_ckpt_steps(ckptdir)\n",
    "# ckpt_step = max(ckpt_step_list[-1:])\n",
    "# ckpt_path = join(ckptdir, f\"model_epoch_{ckpt_step:06d}.pth\")\n",
    "ckpt_path = join(savedir, f\"model_final.pth\")\n",
    "unet.load_state_dict(torch.load(ckpt_path))\n",
    "print(\"load ckpt from \", ckpt_path)\n",
    "CNN_precd = EDMCNNPrecondWrapper(unet, sigma_data=0.5, sigma_min=0.002, sigma_max=80, rho=7.0)\n",
    "CNN_precd.eval().to(device)\n",
    "CNN_precd.requires_grad_(False);\n",
    "# figdir = join(savedir, \"gradient_maps\")\n",
    "# jacobdir = join(savedir, \"jacobian_store\")\n",
    "# os.makedirs(figdir, exist_ok=True)\n",
    "# os.makedirs(jacobdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dda69c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/orX8NRaPLr0Ca9NJDp+T5jRjJz2H0rvdO/4Vjr2q2mlWXh7WFuLuVY0dbknYWOOmeQKAPLoopJ5UiiRnkchVVRkk+gro/EPgDxF4W0y11DV7HyLe5OEO8Eg4zgjqPxr2S90L4d/CG+ivrrz77U8b4IH+Yqezeg+teY+Pfilq/jdmtnVbbTAwKW6jnjoSe5oA4aGGS4mSGFGeR2CqqjJJPava/C2haJ8LIIvEHiu62600Za2sEALrkYBI9fftXjulajNpGrWmowAGW2lWVAemQc13fiPxn4O8UXzanfeH79dQmUec8dzhSwHYZoA5DxNr914k1y41C5uLiUO58oTuWKJnhayO9WtRktJtQnksIHgtGcmKN23FV7AnvVdiG27VwQMH3oA/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAEyklEQVR4AX2WuS/1XRDH7fsSYl9iT+xBQoFEbogQIahoKLX+ACWVRqPUKCQKiVAIiURjixD7HgrEEt7gta/v53fnPuO4D+8pzp0z853vzJkz5/yui4uLi5ubm6urKwJDBDTmEqUO0Xt4eKCRWU0qKI+AHaROjOZSHCSqsIjGCSNLNYG0NI4fNf4iKEwFBZqaH2RUTgVRT4QfHOxmU68wlDpQuru727HfWRwq+8GoLII4q9JpKVlCiiCyIK2zMn2QPz8/dUZQgOgFjFKWInDaoaGhubm5/v7+Ly8vBwcHe3t7ACyMRFMW8Td5VaPUThqyjoyMbGtr29/fv7m5OTs76+7u9vPz+5Xk/4NhFYDMsISFhXV0dJyfn7+/v398fJD10NBQWlqanMHXDTATtJfxq5TWTv/Uytr2nxoSIygoqKCgoKWlJTAwEP3r6yslSk9Pr6+vJxhe1hmIg1AgQ0wpEYB6eXkxM7CiMcGenp4JCQkQNTU1xcbGki+M4Hd2dmZnZ1dWVoTWA08dqEAkJibW1dWVlZVtbGzgGRwc/Pj4eHJycnV1tb6+fn9/T6GjoqLQZ2dnEwAMXvje3t6Oj4+PjIzgeHp6SqJWTsouNc3Ly2ttbWVmvL29kTvONAl7h/r6+ppIAQEBKBnh4eExMTEwPD09bW9vj46Ojo2NbW5u3t3dKe1XiYjGruPi4srLy9lvf39/UlLSv/ZBGMLTGJWVlSkpKQhkJxRHR0eHh4dra2uUZWpqihbiqCV3AXhYuzCOgf4dHh4mTE9PD1C4Hh4eiIKMkmavqanB5fn5may3tramp6cnJycJcHx8DKOUQY7XsWT7mg4Cw8fHp6SkBDMmyYIZfU5OzsLCAoWidGQ6ODhYW1sbEhIiVqiFXV0cS0opEjMsYnaCoiR3zobcKRfzxMQE7L6+vphM8N+ymzhgwM1Em1CSoD6lpaVgOCc28Y99UCXTUWRxFFnumpW0aC3JGLohLhHpUxa6qK+vjyaOj4+nkgb2B1E4HV0k6Tuh5KyoMkdis9nInbxR0jY0q56kesHIED0CenZgdRGSrBHMSJijo6MbGxurq6sLCwupOAHYFvfOhGkAdRc2wVh9wpqFOYtPZmYmjd/c3JyVlQUvN3lubm5mZoabrGBBqrsuVbAC/J0ODcoDwDMgL8HFxQVlmZ+f5/bt7u5yq4mnMcTdJFETgkT6NtMzqampnZ2dS0tLl5eXy8vLvb293C8TpJ4iMItgYr5ksenM16Orq4ty86TA3tDQoP0ORlrLFIRI9CILlezSEcZU5efn83LRDDxexcXFPN0C0qYWMErTi6UwOlm/tSmkVJ9nh37nEtHptCYVR0ApnsiYJCRKBKovBwAYBh4SkKJB+HptxAcEdV9cXExOTuYj3t7eXlVVxW3HjW2xiaKiooGBgdXVVXlihQhfuLgxAOgIwMKG9atNNYBcVyJVVFTwPPDG8fXgSDIyMnhZmfmk4AkjQwOwdax8KuARqxBabQROtKKilKD5attsNi4aJeKR4MB5MLgBERERBOAzgF4CaBhxVL3JKcyOvzAsJKRDa/8xNSL/rTEdnTEm2uT9TVZ/0xFZhnixIQQ3aT4xOFTGV0E0VMyC2oc6e3t748WSWR31bISQD+J/mhGycp3tmZQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "noise_init = torch.randn(1, 1, 32, 32, generator=torch.Generator(device=device).manual_seed(seed), device=device)\n",
    "x_out_i = edm_sampler(CNN_precd, noise_init, num_steps=25, \n",
    "            sigma_min=0.002, sigma_max=12.5, rho=7, return_traj=False)\n",
    "to_imgrid(((x_out_i.cpu()+1)/2).clamp(0, 1), padding=1)#.resize((64, 64), resample=PIL.Image.Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e7847eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attn_resolutions': [],\n",
      " 'augment_dim': 0,\n",
      " 'channel_mult': [1, 2, 3, 4],\n",
      " 'channels': 1,\n",
      " 'decoder_init_attn': True,\n",
      " 'dropout': 0.0,\n",
      " 'img_size': 32,\n",
      " 'label_dim': 0,\n",
      " 'layers_per_block': 1,\n",
      " 'model_channels': 64}\n",
      "total number of trainable parameters in the Score Model: 15916417\n",
      "total number of parameters in the Score Model: 15916417\n",
      "load ckpt from  /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_wide64_noattn/model_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1275571/4197375733.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(ckpt_path))\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooq7pGlXet6rbabYxmS5uHCIvuaAK1vbzXdxHb28TyzSMFREGSxPQAV0/in4ea74P0qxv8AVokiS8O1UDAspxnBH0r2DTvDXhb4L2UGs69P9t1eRQI4woOG7lP8a8k+IHxB1Hx5qgmuB5NlCT9ntx/CPU+poA5W1tZ726itraJpZ5WCIiDJYnoBXvvhbw3onwk0hPEnie5xrLxkxWmQWBI+6B3Pv2rxbwn4il8KeJLTWIYI53tzny5OhzVzxz4xuPG/iBtVnhEGUVFiViVXAxxmgCt4s8UX/ivW5768uZ5Ii7eRFLIW8pCeFFYVFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAC5klEQVR4Ae2Wy0tqURTG89GDyqD3NKmJSlgDCakIKquJgwYVDRzov9FcbBL2ByRICA3SHgiOGlSDBj1wpHCDBikVlRZa2sNb90cnzo2DJ08hNGkPZJ219/7WOt/61jpWVPyun2ZAVcYEmpqaOjs7c7nc6elpNpsVkMsTQKPR6PX64eHh/v7+6+vrjY2NnZ2dsqVeXV1tNBrn5+cvLy8LhcL9/b3H4xHRtaL1PUOr1ULL4uLi4OBgVVXV6+trZWUlThHtvyW6lBu1tbUmk2l2dnZgYACWQH95efnztlQqFY/KoYqcrKmpmZiYWF1dvbi4gBlh3d3dwRXvVOTCl1wdHR2U1Ov1Xl1dPT8/g84vqc/NzVEPuBLRvkPR5OTkyMgIaXZ1dSFNqCBAPB73+Xybm5uJRAKixABfMNRqdXNzs91uj0Qi6XT679tKpVL7+/uBQMDlcrW0tED9FxAlRxsbGx0Ox8HBQSaTIeXHx8dkMrm+vu50Oru7uwkvOf/+qDAmJUUqsVgMXFKHcdD9fv/o6GhxXNGrJABnYNztdgNN7lCcz+fD4TAqEnHkDLUStba3tzMDhoaGBBSuPD097e3twb4cruhXpCKdTmcwGMxmM8wcHR0hle3t7ePj45ubGxFIzigRAHKYAWNjY319fagbipgEu7u7hKEYcqAf/Z8FaG1tBXdmZsZqtaJ3rkE9Mj8/P1eI/jFSEXt6ejoUCgFHYcmdOby8vDw1NdXQ0FDktHIXtNTX1/f09KysrDBnqCf5kjiN2tvbK6t35QEoqc1mCwaDJycnfJ5ub28ZAwsLC21tbcpBZE9SSYvFgv7IGmYeHh7QzPj4eBkSF2IyC5eWlkAX2nVtbY1uqqurk82o1IZ0gKAc3oB86SbW1tbW4eEhRJXCkd2XBmBMRqNRhgHoQnlpLmxZgFIb0j5ALWjx7OyMhqIAlJdKlAL5bF86vtEog5PJDktAM5n5l/DND8hncX/33hmA8H814YPdb9uFjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading config \n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_noattn\"\n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_wide64_noattn\"\n",
    "# expname = \"MNIST_UNet_CNN_EDM_shallow\"\n",
    "savedir = join(saveroot, expname)\n",
    "ckptdir = join(savedir, \"ckpts\")\n",
    "sample_dir = join(savedir, \"samples\")\n",
    "config = edict(json.load(open(f\"{savedir}/config.json\")))\n",
    "# args = edict(json.load(open(f\"{savedir}/args.json\")))\n",
    "pprint(config)\n",
    "unet = create_unet_model(config)\n",
    "# ckpt_step_list = find_all_ckpt_steps(ckptdir)\n",
    "# ckpt_step = max(ckpt_step_list[-1:])\n",
    "# ckpt_path = join(ckptdir, f\"model_epoch_{ckpt_step:06d}.pth\")\n",
    "ckpt_path = join(savedir, f\"model_final.pth\")\n",
    "unet.load_state_dict(torch.load(ckpt_path))\n",
    "print(\"load ckpt from \", ckpt_path)\n",
    "CNN_precd = EDMCNNPrecondWrapper(unet, sigma_data=0.5, sigma_min=0.002, sigma_max=80, rho=7.0)\n",
    "CNN_precd.eval().to(device)\n",
    "CNN_precd.requires_grad_(False);\n",
    "# figdir = join(savedir, \"gradient_maps\")\n",
    "# jacobdir = join(savedir, \"jacobian_store\")\n",
    "# os.makedirs(figdir, exist_ok=True)\n",
    "# os.makedirs(jacobdir, exist_ok=True)\n",
    "seed = 1234\n",
    "noise_init = torch.randn(1, 1, 32, 32, generator=torch.Generator(device=device).manual_seed(seed), device=device)\n",
    "x_out_i = edm_sampler(CNN_precd, noise_init, num_steps=25, \n",
    "            sigma_min=0.002, sigma_max=12.5, rho=7, return_traj=False)\n",
    "to_imgrid(((x_out_i.cpu()+1)/2).clamp(0, 1), padding=1)#.resize((64, 64), resample=PIL.Image.Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c853f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attn_resolutions': [],\n",
      " 'augment_dim': 0,\n",
      " 'channel_mult': [1, 2, 3, 4],\n",
      " 'channels': 1,\n",
      " 'decoder_init_attn': True,\n",
      " 'dropout': 0.0,\n",
      " 'img_size': 32,\n",
      " 'label_dim': 0,\n",
      " 'layers_per_block': 1,\n",
      " 'model_channels': 16}\n",
      "total number of trainable parameters in the Score Model: 1000801\n",
      "total number of parameters in the Score Model: 1000801\n",
      "load ckpt from  /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/MNIST_UNet_CNN_EDM_4blocks_noattn_smalllr_longtrain/model_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1275571/3819551461.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(ckpt_path))\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwCiir+i6Pea9q9tplhEZLm4cIg+vc+1AFS3t5rq4jt7eJpZpGCoiDJYnoAK6nxV8O9a8HaTp9/qqxoLw7RGGyyHGcH8K9csfDHhX4MWMOs65N9u1hlHlRhQcP/sf415D488eah461cXV0BFbxZWCBeiD39TQBzdlZXOpXsNnZwvNcTMEjjQZLE9BX0B4Y8P6J8HdGGveJLof21NF8lqrAkEj7qjv9a8U8I+J7jwh4ig1m1t4p5YQwCS9DkYpfFviq/wDGGuy6rf4WRwAsaH5UA7CgBPFfia+8Va5Pf3dxPJGXbyY5XLeUmeFFYdFFAH//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAEQklEQVR4AYWWyyvtbRTH3Z3kErknSYTMRAmxmSBmiKkJJf+FMjMwUIwdygwDTkdiwqlzDkWSFAMk5JZrvC7vZ/v+zvL4nXPe9xk8+7vW+q7vevZ6LnuHhIaGhoSEMAuAZbpAUc3mN6d5XCA15jC8fLy+vgooIKrNimICIDDk+VuWCDYH2RgmZ0IGfie4ZGGX42LvG1gCMQsLMKuSzUb2ATFxussFe2v/Y9icvjSfNGZYWFhaWlpxcfHt7e329vbp6alSIozqVjIsknGo54ZkMkdGRubk5LS0tNTW1h4fH3/+/Pnr168eUx/MGtL6ZX1wqhguF4A/ffpUWFg4MDBwfn7+/Px8c3PT399vOsFvQI6tRabCOGVKVKbLxx8eHp6bm4t6IBCIjo6G49Igey2yGpLWLCcJFrVkCGA6U1BQ0NHRUV1dHRER8fLycnd3NzMzMzc3x5Z4ZJJNVJhZAL8BK2nkmJiYurq60dHRg4ODf97G09PT7u5uV1dXUlLSm8abjkkIWOB30zysLiMjo76+fmho6Ojo6PHxEemHh4cfP3709PRkZ2dDMJ33PbClAdQWSAIW0hdvb2+vqKgoKirKy8tLSUmhM2zst2/fxsbG5ufnOUVqjpdrpXwAUZ+H/czMzGxra0Pl7OyMA4PExcXF9+/fBwcHGxsb4+PjWYESLd2vYqJatUwwmampqb29vZubm+wkbaHt9Gd6erqzs5O2wHwXVbK20BQNiGezQGxsbFNTE4rosnYKHB4e0hMWDsGaDrbhLc50DcDwYr9AVFRUeXn5+Pg4PWE/afr9/f3U1FRDQ4ORSZG06QgEW6ahnQEbFayNYj+bm5vZWKRxMrOTs7OzS0tLLlmYFIY0mT+8RRYQ1ZITEhK4UBxN+rOysjI5ObmwsLCzs8O7RorRXPyhgOnK6yZw+2tqalpbW3kmodF9lr+8vLy+vs42iC9dV93F3j2wggDCMrOysqqqqjj1ZWVlycnJ7OTl5eX+/j5bzU5oHabuKrghr0VuTcKYyHFsOPWlpaVxcXEsnMfyy5cvtJ6zD4EBk4G0ZtXA7/N4YSNxoTjyvASLi4ssmVZw8PkNGR4eLikp4VFTvk8aU8PUPZMPVZaNenp6end399ra2vX1Neqsd3V1ta+vj4vqSgi7s6uDX6b/yCcmJtIWukGXGZz3iYmJQCDgE3JNacnjqxEMWZgYy+dl//nzJ9IcGJ6wkZGRyspKzhJRk/CB302X/OF/ETeW5yw/P/9tC1/54eZdc0+kMm0bBXAaADO0aIFgAQuz6pOTk42NDR73vb09HjI9bRDIMZqbH9T7zxH8fhCUzNHkrwennn3mlm5tbXFdr66uTNqkVNJMH3CjwQJuvltP2MqL6TpdDM1Mt97774MYKgaVIWwe1wRDMKE3+rspv/jeHogtF2GAsGZFbcYJVshq+IA4OGH6K/uofzP/t4bU/wVnxp+PXJsxyAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading config \n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_noattn\"\n",
    "expname = \"MNIST_UNet_CNN_EDM_4blocks_noattn_smalllr_longtrain\"\n",
    "# expname = \"MNIST_UNet_CNN_EDM_shallow\"\n",
    "savedir = join(saveroot, expname)\n",
    "ckptdir = join(savedir, \"ckpts\")\n",
    "sample_dir = join(savedir, \"samples\")\n",
    "config = edict(json.load(open(f\"{savedir}/config.json\")))\n",
    "# args = edict(json.load(open(f\"{savedir}/args.json\")))\n",
    "pprint(config)\n",
    "unet = create_unet_model(config)\n",
    "# ckpt_step_list = find_all_ckpt_steps(ckptdir)\n",
    "# ckpt_step = max(ckpt_step_list[-1:])\n",
    "# ckpt_path = join(ckptdir, f\"model_epoch_{ckpt_step:06d}.pth\")\n",
    "ckpt_path = join(savedir, f\"model_final.pth\")\n",
    "unet.load_state_dict(torch.load(ckpt_path))\n",
    "print(\"load ckpt from \", ckpt_path)\n",
    "CNN_precd = EDMCNNPrecondWrapper(unet, sigma_data=0.5, sigma_min=0.002, sigma_max=80, rho=7.0)\n",
    "CNN_precd.eval().to(device)\n",
    "CNN_precd.requires_grad_(False);\n",
    "# figdir = join(savedir, \"gradient_maps\")\n",
    "# jacobdir = join(savedir, \"jacobian_store\")\n",
    "# os.makedirs(figdir, exist_ok=True)\n",
    "# os.makedirs(jacobdir, exist_ok=True)\n",
    "seed = 1234\n",
    "noise_init = torch.randn(1, 1, 32, 32, generator=torch.Generator(device=device).manual_seed(seed), device=device)\n",
    "x_out_i = edm_sampler(CNN_precd, noise_init, num_steps=25, \n",
    "            sigma_min=0.002, sigma_max=12.5, rho=7, return_traj=False)\n",
    "to_imgrid(((x_out_i.cpu()+1)/2).clamp(0, 1), padding=1)#.resize((64, 64), resample=PIL.Image.Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa1c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c255908",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Xtsr = torch.load(\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/DiffusionSpectralLearningCurve/wordnet_render_dataset/ffhq-32x32.pt\")\n",
    "ref_x_raw = data_Xtsr[77]\n",
    "ref_x = (ref_x_raw - 0.5) / 0.5\n",
    "# ref_x = ref_x.flatten()\n",
    "plt.imshow(ref_x_raw.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5022e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgshape = (3, 32, 32)\n",
    "figdir = join(savedir, \"gradient_maps\")\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "ckpt_step_list = find_all_ckpt_steps(ckptdir)\n",
    "for ckpt_step in ckpt_step_list:\n",
    "    ckpt_path = join(ckptdir, f\"model_epoch_{ckpt_step:06d}.pth\")\n",
    "    CNN_precd.load_state_dict(torch.load(ckpt_path))\n",
    "    device = \"cuda\"\n",
    "    CNN_precd = CNN_precd.to(device).eval()\n",
    "    CNN_precd.requires_grad_(False);\n",
    "    save_dict = {}\n",
    "    noise_z = torch.randn(imgshape, generator=torch.Generator().manual_seed(0))\n",
    "    for sigma in tqdm([0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]):\n",
    "        x_probe = ref_x + sigma * noise_z\n",
    "        jacobian = compute_jacobian(CNN_precd, x_probe, sigma, device, pbar=False)\n",
    "        save_dict[(sigma, \"jacobian\")] = jacobian.cpu()\n",
    "        save_dict[(sigma, \"x_probe\")] = x_probe\n",
    "    torch.save(save_dict, join(jacobdir, f\"jacobian_ref_img_w_noise_step{ckpt_step:06d}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
