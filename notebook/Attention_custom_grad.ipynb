{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/notes/extending.html#extending-autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOp(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k):\n",
    "        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n",
    "        ctx.save_for_backward(q, k, w)\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dw):\n",
    "        q, k, w = ctx.saved_tensors\n",
    "        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n",
    "        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n",
    "        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n",
    "        return dq, dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "result = AttentionOp.apply(\n",
    "    torch.randn(2, 5, 192, device=device),\n",
    "    torch.randn(1, 5, 192, device=device)\n",
    ")\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class AttentionOp_custom(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # No additional context needed for functorch transforms.\n",
    "        q, k = inputs\n",
    "        w = output\n",
    "        ctx.save_for_backward(q, k, w)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(q, k):\n",
    "        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n",
    "        # ctx.save_for_backward(q, k, w)\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dw):\n",
    "        q, k, w = ctx.saved_tensors\n",
    "        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n",
    "        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n",
    "        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n",
    "        return dq, dk\n",
    "# --- Hiding the signature ---\n",
    "# The goal is that the user sees the \"apply\" method as accepting only (q, k)\n",
    "# rather than (cls, q, k) with a hidden 'ctx'.\n",
    "\n",
    "# First, grab the original apply method.\n",
    "_original_apply = AttentionOp_custom.apply\n",
    "\n",
    "# Define a new wrapper that takes only (q, k) and calls the original.\n",
    "def _apply_wrapper(q, k):\n",
    "    return _original_apply(q, k)\n",
    "\n",
    "# Set the __signature__ of our wrapper to expose only q and k.\n",
    "_apply_wrapper.__signature__ = inspect.Signature(parameters=[\n",
    "    inspect.Parameter(\"q\", inspect.Parameter.POSITIONAL_ONLY),\n",
    "    inspect.Parameter(\"k\", inspect.Parameter.POSITIONAL_ONLY),\n",
    "])\n",
    "\n",
    "# Replace the original apply with our wrapped version.\n",
    "AttentionOp_custom.apply = _apply_wrapper\n",
    "\n",
    "# --- Example usage ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "result = AttentionOp_custom.apply(\n",
    "    torch.randn(2, 5, 192, device=device),\n",
    "    torch.randn(1, 5, 192, device=device)\n",
    ")\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
